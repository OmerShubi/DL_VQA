defaults:
  - hydra/job_logging: disabled
  - hydra/output_subdir: null
hydra:
  output_subdir: null
  run:
    dir: logs/hydra
main:
  experiment_name_prefix: my_exp
  seed: 1
  num_workers: 32 #todo increase?
  paths:
#    base_path: '/datashare'
#    base_path: './small_data'
    base_path: './zip'
    logs: 'logs/'
    vocab_path: './vocab.json'
#    vocab_path: './vocab_small.json'
  train_paths:
    vqaDataset: './vqaDataset_train.pkl'
#    vqaDataset: './vqaDataset_train_small.pkl'
    questions: 'v2_OpenEnded_mscoco_train2014_questions.json'
    answers: 'v2_mscoco_train2014_annotations.json'
    imgs: 'train2014'
    processed_imgs: './processed_train_images.h5'
#    processed_imgs: './processed_train_images_small.h5'
  val_paths:
    vqaDataset: './vqaDataset_val.pkl'
#    vqaDataset: './vqaDataset_val_small.pkl'
    questions: 'v2_OpenEnded_mscoco_val2014_questions.json'
    answers: 'v2_mscoco_val2014_annotations.json'
    imgs: 'val2014'
    processed_imgs: './processed_val_images.h5'
#    processed_imgs: './processed_val_images_small.h5'
train:
  text:
    question_features: 1024
    embedding_features: 300
    dropout: 0.3
    num_lstm_layers: 1
    bidirectional: False
  image:
    image_features: 64
    kernel_sizes: [5,5,5,5]
    dropout: 0.3
    num_channels: [3, 6, 16, 32, 64] # last entry should match image_features

  attention:
    hidden_dim: 512
    glimpses: 2
    dropout: 0.3
  classifier:
    hidden_dim: 1024
    dropout: 0.3

  max_answers: 3000
  image_size: 224 # scale shorter end of image to this size and centre crop
  central_fraction: 0.875  # only take this much of the centre when scaling and centre cropping

  num_epochs: 100
  batch_size: 128
  save_model: True
  lr: # TODO what are reasonable values?
    lr_value: 1e-3
    lr_decay: 15
    lr_gamma: 0.1
    lr_step_size: 3